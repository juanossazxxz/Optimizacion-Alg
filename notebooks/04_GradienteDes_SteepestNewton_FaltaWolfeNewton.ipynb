{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKgbvmCZRJX8"
      },
      "source": [
        "<center><h1> Optimizacion </h1></center>\n",
        "<center><h2>Gradiente descendente</h2></center>\n",
        "\n",
        "En este laboratoria se implementara el gradiente descendiente:\n",
        "\n",
        "1. El metodo Steepest Decent con las condiciones de Armijo and Wolfe<br>\n",
        "<br>\n",
        "2. Metodod de Newton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8GMFJmwRJX-"
      },
      "source": [
        "Usaremos el paquete [autograd](https://github.com/HIPS/autograd) para calcular el gradiente y la Hessiana de nuestra funcion objetivo. <i>autograd</i> usa diferenciacion automatica. Basado en el lab del profesor "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l29m5qiwRJX-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Use np.linalg.solve to solve A^-1*b\n",
        "\n",
        "# to calculate the gradient and Hessian of the objective function\n",
        "from autograd import grad, hessian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQfAAYKpRJX_"
      },
      "outputs": [],
      "source": [
        "def objective_function(x): \n",
        "    f = 0.5 * (x[0]**2 + 20 * x[1]**2)\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrzOAJWlRJX_"
      },
      "outputs": [],
      "source": [
        "def rosenbrock(x):\n",
        "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHO76CSURJX_"
      },
      "outputs": [],
      "source": [
        "def polynomial(x): \n",
        "    return - 1.0 / 6 * x[0]**6 + 1 / 4 *x[0]**4 + 2 * x[0]**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSa4XQCsRJYA"
      },
      "outputs": [],
      "source": [
        "def gradient_function(func, x): \n",
        "    return grad(func)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6jcUxg7RJYB"
      },
      "outputs": [],
      "source": [
        "def hessian_function(func,x):\n",
        "    return hessian(func)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q79dYGtRJYB"
      },
      "outputs": [],
      "source": [
        "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n",
        "def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n",
        "    \n",
        "    xCur = x0.copy()\n",
        "    fcur = function(xCur)\n",
        "    gCur = gradient_function(function,xCur)\n",
        "    \n",
        "    # norm of the gradient at initial point for optimality condition\n",
        "    nmg0 = np.linalg.norm(gradient_function(function, x0))\n",
        "    \n",
        "    # count number of steps taken by method\n",
        "    it = 0\n",
        "    \n",
        "    # accumulate number of iterations needed by linesearch algorithm in every step\n",
        "    ls_iter = 0\n",
        "    \n",
        "    # store iterates for plotting\n",
        "    xIter=[]\n",
        "    xIter.append(x0)\n",
        "    \n",
        "    while (np.linalg.norm(gCur)>tol*min(1,nmg0)): \n",
        "\n",
        "        it=it+1\n",
        "        \n",
        "        # calculate step-length\n",
        "        if (stepRule=='armijo'):\n",
        "            alpha, ls_ = armijo(function,xCur,-gCur, c1, alpha0)\n",
        "        elif (stepRule=='wolfe'):\n",
        "            alpha, ls_ = wolfe(function,xCur,-gCur, c1, c2, alpha0)\n",
        "        else:\n",
        "            alpha = 0.01\n",
        "            ls_ = 1\n",
        "        ls_iter = ls_iter + ls_\n",
        "        \n",
        "        # update current point\n",
        "        #x(k+1) = x(k) + alpha*direction\n",
        "        xCur = xCur + alpha*(-gCur)\n",
        "        #print(xCur, alpha, direction)\n",
        "        # add your code here\n",
        "        fcur = function(xCur)\n",
        "        gCur = gradient_function(function,xCur)\n",
        "\n",
        "        xIter.append(xCur)\n",
        "\n",
        "    return xCur, fcur, it, xIter, ls_iter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k8RaC1oRJYC",
        "outputId": "3de1b8fc-89cf-43d7-c041-97412ab37a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-5356ffb7f34a>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    direction = # add your code here\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# input: (objective function, initial guess, optimality tolerance)\n",
        "def newton_descent(function, x0, stepRule='no', tol=0.01): \n",
        "\n",
        "    xCur = x0\n",
        "    fcur = function(xCur)\n",
        "    gCur = gradient_function(function,xCur)\n",
        "    hCur = hessian_function(function,xCur)    \n",
        "    \n",
        "    # norm of the gradient at initial point for optimality condition\n",
        "    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n",
        "    \n",
        "    # count number of steps taken by method    \n",
        "    it = 0\n",
        "    \n",
        "    # store iterates for plotting    \n",
        "    xIter=[]\n",
        "    xIter.append(x0)\n",
        "    \n",
        "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n",
        "        \n",
        "        it=it+1\n",
        "        \n",
        "        # calculate descent direction\n",
        "        direction = # add your code here\n",
        "        \n",
        "        # calculate step-length or use natural length of 1\n",
        "        if (stepRule ==  # add your code here\n",
        "            alpha = # add your code here\n",
        "        \n",
        "        # update current point\n",
        "\n",
        "        # add your code here\n",
        "        \n",
        "        xIter.append(xCur)\n",
        "\n",
        "    return xCur, fcur, it, xIter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoHyktJ4RJYC"
      },
      "outputs": [],
      "source": [
        "def armijo(function, xcur, searchdirection, c1, alpha0): \n",
        "    #f(x+alpha p) <= f(x) + c1 alpha grad(f).p\n",
        "    \n",
        "    alpha = alpha0\n",
        "    xnew = xcur + alpha * searchdirection\n",
        "    fcur = function(xcur) #phi(0)\n",
        "    fnew = function(xnew) #phi(alpha)\n",
        "    gradientCur = gradient_function(function,xcur) #phi'(0) = gradientCur.T@searchdirection\n",
        "    numiter = 0\n",
        "    # check for Armijo condition\n",
        "\n",
        "    while (fnew > fcur + alpha*c1*gradientCur.T@searchdirection):\n",
        "        numiter += 1\n",
        "        alpha = alpha/2.0\n",
        "        xnew = xcur + alpha * searchdirection\n",
        "        fnew = function(xnew)\n",
        "\n",
        "    return alpha, numiter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgR4Uc5xRJYD"
      },
      "outputs": [],
      "source": [
        "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n",
        "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n",
        "    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n",
        "    \n",
        "    alpha = alpha0\n",
        "    xnew = xcur + alpha * searchdirection\n",
        "    fcur = function(xcur)\n",
        "    fnew = function(xnew)\n",
        "    gradientCur = gradient_function(function,xcur)\n",
        "    gradientNew = gradient_function(function,xnew)\n",
        "    numiter = 0\n",
        "    \n",
        "    lb = 0\n",
        "    ub = np.inf \n",
        "    \n",
        "    # check for Wolfe conditions\n",
        "    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n",
        "    while True: \n",
        "        numiter += 1\n",
        "        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n",
        "            ub = alpha\n",
        "            alpha = 0.5 * (lb + ub)\n",
        "        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n",
        "            lb = alpha\n",
        "            if np.isinf(ub):\n",
        "                alpha = 2 * lb\n",
        "            else:\n",
        "                alpha = 0.5 * (lb + ub)\n",
        "        else:\n",
        "            break\n",
        "        xnew = xcur + alpha * searchdirection\n",
        "        fnew = function(xnew)\n",
        "        gradientNew = gradient_function(function,xnew)\n",
        "\n",
        "    return alpha, numiter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L10qO3T9RJYD"
      },
      "source": [
        "## Solving quadratic unconstrained problem\n",
        "###  Steepest descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQ3YP9k7RJYD",
        "outputId": "cad9500a-ddab-40b1-d5dd-09067bc62fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method                                           xopt                 fopt              iter           ls_iter    \n",
            "Armijo with default tol = 0.01           [0.0069, -0.0001]          2.3644e-05           44              15             \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([0.9, 0.1])\n",
        "c1 = 0.5 \n",
        "c2 = 0.6 \n",
        "alpha0 = 0.125 \n",
        "tol = 1e-5\n",
        "\n",
        "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
        "\n",
        "# using steepest descent with Armijo condition for linesearch\n",
        "xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'armijo', c1, c2, alpha0, tol)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(objective_function, x0, 'wolfe', c1, c2, alpha0, tol)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-P__Y20RJYE"
      },
      "source": [
        "### Newton's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCyaJ_eiRJYE"
      },
      "outputs": [],
      "source": [
        "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(objective_function, x0)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
        "print(\"\\n\")\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(objective_function, x0, tol=0.0001)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUpm-vT6RJYE"
      },
      "source": [
        "## Solving Rosenbrock's function\n",
        "Try also different starting points and play around with the tuning factors.\n",
        "\n",
        "### Steepest descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "k68UW1ueRJYE",
        "outputId": "72c2de2a-fbb7-4458-ad21-dc83ec13efbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method                                           xopt                 fopt              iter           ls_iter    \n",
            "Armijo with default tol = 0.01           [0.9892, 0.9784]          1.1708e-04           964             5156           \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "x0 = np.array([0.9, 0.1])\n",
        "c1 = 0.5 \n",
        "c2 = 0.6 \n",
        "alpha0 = 0.125 \n",
        "tol = 0.0001\n",
        "\n",
        "print (\"{:<40} {:^20} {:^20} {:^15} {:^15}\".format('Method','xopt','fopt','iter','ls_iter'))\n",
        "\n",
        "# using steepest descent with Armijo condition for linesearch\n",
        "xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'armijo', c1, c2, alpha0, tol)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Armijo with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "#xopt, fopt, it, xIter, ls_iter = steepest_descent(rosenbrock, x0, 'wolfe', c1, c2, alpha0, tol)\n",
        "#print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d} {:<15d}\".format('Wolfe with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it,ls_iter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfPiaWoGRJYE"
      },
      "source": [
        "### Newton's method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ebUMrsvmRJYF"
      },
      "outputs": [],
      "source": [
        "print (\"{:<40} {:^20} {:^20} {:^15}\".format('Method','xopt','fopt','iter'))\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],xopt[1],' ',fopt,it))\n",
        "print(\"\\n\")\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(rosenbrock, x0, tol=0.0001)\n",
        "print (\"{:<40} [{:^6.4f}, {:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with tol = 0.0001',xopt[0],xopt[1],' ',fopt,it))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2z3xo-URJYF"
      },
      "source": [
        "## Simple polynomial function\n",
        "Here, a simple polynomial function is considered. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYoGBVPMRJYF"
      },
      "outputs": [],
      "source": [
        "x0 = np.array([1.0])\n",
        "\n",
        "xopt, fopt, it, xIter, ls_iter = steepest_descent(polynomial, x0, 'armijo', c1, c2, alpha0)\n",
        "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Steepest descent with default tol = 0.01',xopt[0],' ',fopt,it))\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(polynomial, x0, stepRule='armijo')\n",
        "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with Armijo default tol = 0.01',xopt[0],' ',fopt,it))\n",
        "\n",
        "xopt, fopt, it, xIter = newton_descent(polynomial, x0)\n",
        "print (\"{:<40} [{:^6.4f}] {:<8} {:<20.4e} {:<15d}\".format('Newton with default tol = 0.01',xopt[0],' ',fopt,it))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}